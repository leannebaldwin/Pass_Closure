{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../src/pipeline_classes.py\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Featurizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transform incoming df to fit into model\"\"\"\n",
    "   \n",
    "    def __init__(self, cols=None):\n",
    "        \"\"\"INPUT: an optional cols list of columns to select\"\"\"\n",
    "        if cols==None:\n",
    "            self.cols = ['date', 'temp', 'precipitation', 'overcast', 'poor_visibility', 'windy']\n",
    "        else:\n",
    "            self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"tranform incoming training or test\"\"\"\n",
    "        df = X.copy()\n",
    "        date_column = pd.Series(df.date)\n",
    "        month_day_of_week = pd.DataFrame({\"year\": date_column.dt.year,\n",
    "                                        \"month\": date_column.dt.month, \n",
    "                                        \"day\": date_column.dt.day,\n",
    "                                        \"dayofweek\": date_column.dt.dayofweek})\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 0] = 'Monday'\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 1] = 'Tuesday'\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 2] = 'Wednesday'\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 3] = 'Thursday'\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 4] = 'Friday'\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 5] = 'Saturday'\n",
    "        month_day_of_week.dayofweek[month_day_of_week.dayofweek == 6] = 'Sunday'\n",
    "        month_day_of_week = pd.get_dummies(month_day_of_week)\n",
    "        month_day_of_week.index = df.index\n",
    "        features = pd.concat([df, month_day_of_week], axis=1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/weather_data_clean.py\n",
    "import pandas as pd\n",
    "\n",
    "def clean_weather_data(filename):\n",
    "    \"\"\"Take ASOS weather data file for Stampede pass and clean it ready for input to model.\n",
    "    Input: txt file\n",
    "    Output: pandas dataframe\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    # Rename two of the columns\n",
    "    data.rename(columns={'valid':'date', 'tmpf':'temp'}, inplace=True)\n",
    "\n",
    "    # Remove the few rows that have a null value for temp\n",
    "    data = data[~data.temp.eq('M')]\n",
    "\n",
    "    # Remove spaces from column names\n",
    "    data.rename(columns=lambda x: x.replace(' ', ''), inplace=True)\n",
    "\n",
    "    # Only use the standard hourly weather reading at 56 mins past each hour\n",
    "    mask = data['date'].apply(lambda x: x[-2:] == '56')\n",
    "    data = data[mask]\n",
    "\n",
    "    # Create a date series to be used in the clean dataframe\n",
    "    date = pd.to_datetime(data['date'])\n",
    "\n",
    "    # Create a temp series to be used in the clean dataframe\n",
    "    temp = data['temp'].apply(float)\n",
    "\n",
    "    # Cast the null value M to zero to enable create of the raw precipitation series cast to floats\n",
    "    data.p01i[data.p01i == 'M'] = 0\n",
    "    raw_precipitation = data['p01i'].apply(float)\n",
    "\n",
    "    # Create a precipitation series to be used in the clean dataframe\n",
    "    precipitation = raw_precipitation.apply(lambda x: True if (x > 0) else False)\n",
    "\n",
    "    # Convert sky coverage data to clear or cloudy and create an overcast series to be used in the clean dataframe\n",
    "    sky_elements = ['skyc1', 'skyc2', 'skyc3']\n",
    "    data.skyc1 = data.skyc1.astype(str)\n",
    "    data.skyc2 = data.skyc2.astype(str)\n",
    "    data.skyc3 = data.skyc3.astype(str)\n",
    "    sky_agg = data[sky_elements].values.tolist()\n",
    "    sky_reduce = [['overcast' if (('BKN' in element) or ('OVC' in element) or ('VV' in element)) else 'clear'\n",
    "                    for element in row] for row in sky_agg]\n",
    "    overcast = pd.Series([True if 'overcast' in row else False for row in sky_reduce])\n",
    "    overcast.index = date.index\n",
    "\n",
    "    # Cast the null value 'M' to 10.00 to enable the creation of a poor visibility series\n",
    "    data.vsby[data.vsby == 'M'] = 10.00\n",
    "    raw_visibility = data['vsby'].apply(float)\n",
    "    poor_visibility = pd.Series([True if value < 0.50 else False for value in raw_visibility])\n",
    "    poor_visibility.index = date.index\n",
    "\n",
    "    # Cast the null value 'M' to 0 to enable the creation of a windy series\n",
    "    data.sknt[data.sknt == 'M'] = 0.00\n",
    "    data.gust[data.gust == 'M'] = 0.00\n",
    "    wind_speed = data['sknt'].apply(float)\n",
    "    gust_speed = data['gust'].apply(float)\n",
    "    wind_df = pd.concat([wind_speed, gust_speed], axis=1)\n",
    "    # Finally apply the function f to enable the creation of the windy column\n",
    "    windy = wind_df.apply(f, axis=1)\n",
    "\n",
    "    \"\"\"Create the cleaned dataframe by concatenating the date, temp, precipitation, overcast, poor_visibility\n",
    "    and windy series\"\"\"\n",
    "    df = pd.concat([date, temp, precipitation, overcast, poor_visibility, windy], axis=1)\n",
    "    df.columns = ['date', 'temp', 'precipitation', 'overcast', 'poor_visibility', 'windy']\n",
    "    cleaned_df = df[(df['date'] > '2006-12-31') & (df['date'] < '2018-04-03')]\n",
    "    return cleaned_df\n",
    "\n",
    "def f(row):\n",
    "    \"\"\"Function to be able to create the windy series with windy being true if wind speed is above 10 knots\n",
    "    or gust speed is above 20 knots\"\"\"\n",
    "    if row['sknt'] >= 10.00:\n",
    "        val = True\n",
    "    elif row['gust'] >= 20.00:\n",
    "        val = True\n",
    "    else:\n",
    "        val = False\n",
    "    return val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../src/pass_data_clean.py\n",
    "import pandas as pd\n",
    "\n",
    "def clean_pass_data(filename):\n",
    "    \"\"\"Take Snoqualmie pass closure data file and clean it ready for input to model.\n",
    "    Input: xlsx file\n",
    "    Output: Pandas dataframe\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(filename, header=[1])\n",
    "\n",
    "    #drop unnamed/unnecessary columns\n",
    "    data.drop(data.columns[[11,12,13,14]], axis=1, inplace=True)\n",
    "\n",
    "    #drop unnecessary secondary incident columns\n",
    "    data.drop(data.columns[[1,8]], axis=1, inplace=True)\n",
    "\n",
    "    #rename 'Incident...' columns to start_time and end_time \n",
    "    data.rename(columns={'INCIDENT START TIMES FOR EACH DIRECTION':'start_time'}, inplace=True)\n",
    "    data.rename(columns={'INCIDENT END TIMES - DIRECTIONAL':'end_time'}, inplace=True)\n",
    "\n",
    "    #use only dates from 2007-01-01 to match with available weather and traffic volume data\n",
    "    df = data[(data['start_time'] > '2006-12-31')]\n",
    "\n",
    "    #rename 'Delay Time Total' to delay\n",
    "    df.rename(columns={'Delay Time Total':'delay'}, inplace=True)\n",
    "\n",
    "    #drop row with nan value in delay\n",
    "    df = df.dropna(subset=['delay'])\n",
    "\n",
    "    #create a westbound pandas series with True if westbound and false if eastbound\n",
    "    westbound = pd.Series([True if value == 'WB' else False for value in df.DIRECTION])\n",
    "\n",
    "    #create a snow pandas series with True if weather description contains sn, false otherwise\n",
    "    snow = df.WEATHER.str.contains('sn', case=False, na=False, regex=True)\n",
    "\n",
    "    #create pandas series for start and end times\n",
    "    start_time = pd.to_datetime(df['start_time'])\n",
    "    end_time = pd.to_datetime(df['end_time'])\n",
    "\n",
    "    #ensure that all the pandas series created have the same index\n",
    "    westbound.index = start_time.index\n",
    "    snow.index = start_time.index\n",
    "    end_time.index = start_time.index\n",
    "\n",
    "    #create cleaned df with the series created\n",
    "    cleaned_df = pd.concat([start_time, end_time, westbound, snow], axis=1)\n",
    "\n",
    "    #rename columns\n",
    "    cleaned_df.rename(columns={0:'westbound', 'WEATHER':'snow'}, inplace=True)\n",
    "\n",
    "    return cleaned_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leanne/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# %load ../src/combine_data.py\n",
    "import pandas as pd\n",
    "#from pass_data_clean import clean_pass_data\n",
    "\n",
    "pass_closure_df = clean_pass_data('Cumulative_Snoqualmie_Pass_Delay_Closures_1992_2018.xlsx')\n",
    "\n",
    "def get_pass_closure(date_time):\n",
    "    \"\"\"take a date_time and check if it is between the start and end times of a closure event\n",
    "    input: datetime\n",
    "    output: boolean\n",
    "    \"\"\"\n",
    "    start_end_times = list(zip(pass_closure_df.start_time, pass_closure_df.end_time))\n",
    "    for row in start_end_times:\n",
    "        if row[0] <= date_time <= row[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def add_pass_closed(df):\n",
    "    \"\"\"take the weather df and add a new column for whether or not the pass is closed at each date_time\n",
    "    input: pandas dataframe\n",
    "    output: pandas dataframe\n",
    "    \"\"\"\n",
    "    df['pass_closed'] = df['date'].map(get_pass_closure)\n",
    "    return df\n",
    "\n",
    "def true_false_to_one_zero(df):\n",
    "    \"\"\"take the combined df and change all the true/false values to 1/0\n",
    "    input: pandas dataframe\n",
    "    output: pandas dataframe\n",
    "    \"\"\"\n",
    "    df[['precipitation', 'overcast', 'poor_visibility', 'windy', 'pass_closed']] = (\n",
    "        df[['precipitation', 'overcast', 'poor_visibility', 'windy', 'pass_closed']] == True).astype(int)\n",
    "    return df\n",
    "\n",
    "def aggregate_data_to_daily(df):\n",
    "    \"\"\"take the combined df and aggregate the data into daily rather than hourly data to be used to train the model\n",
    "    input: pandas dataframe\n",
    "    output: pandas dataframe\n",
    "    \"\"\"\n",
    "    df.index = df.date\n",
    "    daily_df = df.resample(\"D\").agg({'temp':'mean','precipitation':'max', 'overcast':'max', 'poor_visibility':'max', 'windy':'max', 'pass_closed':'max'})\n",
    "    daily_df.dropna(inplace=True)\n",
    "    daily_df.reset_index(inplace=True)\n",
    "    daily_df.rename(columns={'index':'date'}, inplace=True)\n",
    "    return daily_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/model.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pipeline_classes import Featurizer\n",
    "#from weather_data_clean import clean_weather_data\n",
    "#from pass_data_clean import clean_pass_data\n",
    "#from combine_data import get_pass_closure, add_pass_closed, true_false_to_one_zero, aggregate_data_to_daily\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"function to get all the original data that is required to train the model\n",
    "    Output: pandas dataframe to use to train model\"\"\"\n",
    "    weather_df = clean_weather_data('ASOS_stampede_pass/SMP-2.txt')\n",
    "    combined_df = add_pass_closed(weather_df)\n",
    "    combined_df = true_false_to_one_zero(combined_df)\n",
    "    daily_df = aggregate_data_to_daily(combined_df)\n",
    "    return daily_df\n",
    "\n",
    "def get_training_data():\n",
    "    \"\"\"get the training data that used to train the model\n",
    "    Output: X, y used to train the model\"\"\"\n",
    "    df = get_data()\n",
    "    y = np.array(df['pass_closed'])\n",
    "    df= df.drop('pass_closed', axis=1)\n",
    "    X = np.array(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def pass_pipeline():\n",
    "    \"\"\"instantiate a pipeline object\"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('featurizer', Featurizer()),\n",
    "        ('model', RandomForestClassifier(n_estimators=600, \n",
    "                                         max_depth=40))\n",
    "        ])\n",
    "    return pipeline\n",
    "\n",
    "def pickle_pipeline(pipeline, output_name):\n",
    "    \"\"\"Save fitted pipeline to pickle file\"\"\"\n",
    "    with open(output_name, 'wb') as f:\n",
    "        pickle.dump(pipeline, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leanne/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2014-06-11 00:00:00'), 57.36875, 0.0, 1.0, 0.0, 0.0],\n",
       "       [Timestamp('2013-02-18 00:00:00'), 22.46, 0.0, 1.0, 0.0, 0.0],\n",
       "       [Timestamp('2008-11-11 00:00:00'), 40.71826086956521, 1.0, 1.0,\n",
       "        1.0, 0.0],\n",
       "       ...,\n",
       "       [Timestamp('2010-06-07 00:00:00'), 46.348571428571425, 1.0, 1.0,\n",
       "        1.0, 0.0],\n",
       "       [Timestamp('2010-12-20 00:00:00'), 22.837142857142858, 1.0, 1.0,\n",
       "        1.0, 1.0],\n",
       "       [Timestamp('2009-08-14 00:00:00'), 44.96000000000001, 1.0, 1.0,\n",
       "        1.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = pass_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1d40af031c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    221\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    587\u001b[0m                        **fit_params):\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-291e665f96a1>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m\"\"\"tranform incoming training or test\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdate_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         month_day_of_week = pd.DataFrame({\"year\": date_column.dt.year,\n\u001b[1;32m     24\u001b[0m                                         \u001b[0;34m\"month\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdate_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'date'"
     ]
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
